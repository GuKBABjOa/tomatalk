from pydub import AudioSegment
import whisper
import numpy as np
import torch
import json
from audio.query.audio_query import save_statement

audio_buffer = []
statement=""
SAMPLE_RATE = 16000

# âœ… Whisper ëª¨ë¸ ë¡œë“œ (medium ì´ìƒ ê¶Œì¥)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("medium", device="cpu")

async def audio_transcription(websocket):
    """
        ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³  Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

        Args:
            websocket (WebSocket): FastAPI WebSocket ì—°ê²° ê°ì²´

        Returns:    
            None
    """
    global audio_buffer, statement
    await websocket.accept()
    print("âœ… WebSocket ì—°ê²°ë¨!")

    metadata = await websocket.receive_text()
    metadata = json.loads(metadata)
    room_id = metadata.get("room_id", "unknown")
    speaker_id = metadata.get("speaker_id", "unknown")
    round_number = metadata.get("round", 0)
    print(f"ğŸ“Œ ë°©: {room_id}, ë°œì–¸ì: {speaker_id}, ë¼ìš´ë“œ: {round_number}")

    try:
        while True:
            data = await websocket.receive_bytes()

            if not data:
                print("âš  ìˆ˜ì‹ ëœ ë°ì´í„° ì—†ìŒ.")
                break  

            # PCM ë°ì´í„°ë¥¼ Numpy ë°°ì—´ë¡œ ë³€í™˜
            audio_chunk = np.frombuffer(data, dtype=np.int16)
            audio_buffer.append(audio_chunk)

            # 5ì´ˆ ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìŒ“ì´ë©´ ë³€í™˜ ì‹¤í–‰
            if len(audio_buffer) * 4096 / 160000 > 20:
                await process_audio(websocket)
                audio_buffer = []
    except Exception as e:
        print(f"ì˜¤ë””ì˜¤ ë³€í™˜ ì˜¤ë¥˜: {e}")
    finally:
        print("ğŸ”´ WebSocket ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ë³€í™˜ëœ ë°œì–¸ ì €ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        await save_statement(room_id, speaker_id, round_number, statement)  # ğŸ”¹ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ì €ì¥
        await close_websocket_safely(websocket)


async def process_audio(websocket):
    """"
        ì˜¤ë””ì˜¤ ë²„í¼ë¥¼ ì²˜ë¦¬í•˜ê³  Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

        Args:   
            websocket (WebSocket): FastAPI WebSocket ì—°ê²° ê°ì²´

        Returns:
            None
    """
    global audio_buffer 
    global statement

    if not audio_buffer:
        return

    try:
        # PCM ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ NumPy ë°°ì—´ë¡œ ë³€í™˜
        audio_data = np.concatenate(audio_buffer, axis=0)
    except ValueError:
        print("ë²„í¼ ì¡°í•© ì˜¤ë¥˜: ë°ì´í„°ê°€ ì†ìƒë¨")
        audio_buffer = []  # ì´ˆê¸°í™” í›„ ì¢…ë£Œ
        return

    # âœ… NaN ë° Inf ê°’ ì œê±°
    audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=1.0, neginf=-1.0)

    # âœ… ì˜¤ë²„í”Œë¡œìš° ë°©ì§€ (16-bit PCM ë³€í™˜)
    audio_data = np.clip(audio_data, -32768, 32767).astype(np.int16)

    # âœ… ê³µë°± ë°ì´í„° í•„í„°ë§
    if np.abs(audio_data).mean() < 100:
        print("âš  ì¡ìŒ ë˜ëŠ” ê³µë°± ë°ì´í„° ê°ì§€, ë³€í™˜ ìƒëµ")
        audio_buffer = []  # ë²„í¼ ì´ˆê¸°í™” í›„ ì¢…ë£Œ
        return

    
    if len(audio_data) < SAMPLE_RATE:  # 1ì´ˆ ì´í•˜ ë°ì´í„° ë¬´ì‹œ
        return ""
    
    # âœ… PCM ë°ì´í„°ë¥¼ Whisperê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” `numpy` ë°°ì—´ë¡œ ë³€í™˜
    whisper_input = pcm_to_numpy(audio_data.tobytes(), SAMPLE_RATE)
    
    # âœ… Whisper ëª¨ë¸ë¡œ ë³€í™˜ ì‹¤í–‰
    try:
        result = model.transcribe(
            whisper_input,
            language="ko",
            temperature=0.2,
            no_speech_threshold=0.3,
            fp16=False)
        text = result["text"].strip()
        statement+=text

        if text:
            print(f"ë³€í™˜ëœ í…ìŠ¤íŠ¸: {text}")
            await websocket.send_json({"transcription": text})
    except Exception as e:
        print(f"Whisper ë³€í™˜ ì˜¤ë¥˜: {e}")

    # âœ… ì˜¤ë””ì˜¤ ë²„í¼ ì´ˆê¸°í™”
    audio_buffer = []

# PCM ë°ì´í„°ë¥¼ Whisperê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” `numpy` ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def pcm_to_numpy(pcm_data, sample_rate=16000):
    """
    PCM ë°ì´í„°ë¥¼ Whisper ëª¨ë¸ì— ì§ì ‘ ì…ë ¥í•  ìˆ˜ ìˆëŠ” `numpy` ë°°ì—´ë¡œ ë³€í™˜

    Args:
        pcm_data (bytes): PCM ì˜¤ë””ì˜¤ ë°ì´í„°
        sample_rate (int): ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ 16000Hz)

    Returns:
        numpy.ndarray: Whisper ëª¨ë¸ì´ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    # PCM ë°ì´í„°ë¥¼ WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    audio_segment = AudioSegment(
        data=pcm_data,
        sample_width=2,  # 16-bit PCM
        frame_rate=sample_rate,
        channels=1
    )

    # PCM ë°ì´í„°ë¥¼ float32ë¡œ ë³€í™˜
    samples = np.array(audio_segment.get_array_of_samples()).astype(np.float32) / 32768.0

    # Whisperê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ê³ ì • ê¸¸ì´ ìœ ì§€ (10ì´ˆ)
    target_length = sample_rate * 10  # 10ì´ˆ
    if len(samples) < target_length:
        # ë¶€ì¡±í•œ ë¶€ë¶„ì€ 0ìœ¼ë¡œ ì±„ìš°ê¸° (íŒ¨ë”©)
        padded_samples = np.zeros(target_length, dtype=np.float32)
        padded_samples[:len(samples)] = samples  # ê¸°ì¡´ ë°ì´í„° ìœ ì§€
        samples = padded_samples
    else:
        # ê¸¸ë©´ ìë¥´ê¸°
        samples = samples[:target_length]

    return whisper.pad_or_trim(samples)  # Whisperì— ì ì ˆí•œ ê¸¸ì´ ë§ì¶¤

async def close_websocket_safely(websocket):
    """
    WebSocketì´ ì¢…ë£Œë  ë•Œ ì•ˆì „í•˜ê²Œ ë‹«ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        
        if websocket.client_state in ["CONNECTED", "CONNECTING"]:
            print("âœ… WebSocket ì •ìƒ ì¢…ë£Œ ì¤‘...")
            await websocket.close(code=1000, reason="Normal Closure")
            print("âœ… WebSocketì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš  WebSocketì´ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ WebSocket ë‹«ê¸° ì˜¤ë¥˜ ë°œìƒ: {e}")

